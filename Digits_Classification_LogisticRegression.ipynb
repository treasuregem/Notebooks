{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits=load_digits()\n",
    "f,axes=plt.subplots(2,5)\n",
    "axes[0,0].imshow(digits.images[0])\n",
    "axes[0,1].imshow(digits.images[1])\n",
    "axes[0,2].imshow(digits.images[2])\n",
    "axes[0,3].imshow(digits.images[3])\n",
    "axes[0,4].imshow(digits.images[4])\n",
    "axes[1,0].imshow(digits.images[5])\n",
    "axes[1,1].imshow(digits.images[6])\n",
    "axes[1,2].imshow(digits.images[7])\n",
    "axes[1,3].imshow(digits.images[8])\n",
    "axes[1,4].imshow(digits.images[9])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Logistic Regression class\n",
    "class LogisticRegression():\n",
    "    \"\"\" Class for training and using a model for Logistic Regression\"\"\"\n",
    "    \n",
    "    def set_values(self,initial_params,alpha=0.01,max_iter=5000,class_of_interest=0):\n",
    "        \"\"\"Sets the value for\n",
    "        initial_params: The initial parameters to start with\n",
    "        alpha: Step size analogus to the learning rate in Linear Regression\n",
    "        max_iter: Maximum number of iterations to be carried out\n",
    "        class_of_interest: this determines to which class (0 to 9) the image belongs depending \n",
    "                            on the probability value according to concept of one-vs-all\"\"\"\n",
    "        self.params=initial_params\n",
    "        self.alpha=alpha\n",
    "        self.max_iter=max_iter\n",
    "        self.class_of_interest=class_of_interest\n",
    "        \n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        \"\"\"Its a sigmoid function\"\"\"\n",
    "        \n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, x_bar, params):\n",
    "        \"\"\"This function uses sigmoid function to predict the probability\n",
    "        of an output belonging to a particular class\"\"\"\n",
    "        \n",
    "        return self._sigmoid(np.dot(params,x_bar))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _compute_cost(self, input_var, output_var,params):\n",
    "        \"\"\"This computes the log likelihood cost\"\"\"\n",
    "        \n",
    "        cost = 0\n",
    "        for x, y in zip(input_var,output_var):\n",
    "            # y = theta_0 + x_1 * theta_1 + x_2 * theta_2 +....+ x_n * theta_n\n",
    "            # Here x_0 is 1 for theta_0 so x=[ 1, x_0, x_1, x_2, ...., x_n] where x is out input feature\n",
    "            # We need to add 1 in x at the position 0\n",
    "            x_bar = np.array(np.insert(x, 0, 1))\n",
    "            y_hat = self.predict(x_bar, params) # Calculates the predicted value\n",
    "            \n",
    "#             Now, during training, the class of our interest value is the y value which is the target label. \n",
    "#             suppose, if our class_of_interest is 2 (which we provide, to train for 2) and if the y (label) value for \n",
    "#             currosponding x (feature) value is also 2 in dataset then we take y_binary = 1 else y_binary=0 in cost function\n",
    "\n",
    "            y_binary = 1.0 if y == self.class_of_interest else 0.0\n",
    "            cost += y_binary * np.log(y_hat) + (1.0 - y_binary) * np.log(1 - y_hat) \n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, input_var, label, print_iter=5000):\n",
    "        \"\"\"This trains the model using Batch Gradient Descent\"\"\"\n",
    "        \n",
    "        print(f\"Training for digit: {self.class_of_interest}\")\n",
    "        iteration=1\n",
    "        while iteration<self.max_iter:\n",
    "    \n",
    "            if iteration % print_iter == 0:\n",
    "                print(f\"Cost: {self._compute_cost(input_var, label,self.params)}, Iteration: {iteration}\")\n",
    "            \n",
    "            # Iterate over each and every value of the dataset\n",
    "            for i, xy in enumerate(zip(input_var,label)):\n",
    "                x_bar = np.array(np.insert(xy[0],0,1))\n",
    "                y_hat = self.predict(x_bar, self.params)\n",
    "                y_binary = 1.0 if xy[1]==self.class_of_interest else 0.0\n",
    "                gradient = (y_binary - y_hat) * x_bar\n",
    "                self.params += self.alpha * gradient\n",
    "                \n",
    "            iteration += 1\n",
    "        \n",
    "        return self.params\n",
    "    \n",
    "    \n",
    "    def test(self, input_test,label_test):\n",
    "        \"\"\"This function will test the accuracy of the model.\n",
    "        Accuracy = (total classification / correct classification)\"\"\"\n",
    "        \n",
    "        self.total_classification=0\n",
    "        self.correct_classification=0\n",
    "        \n",
    "        for x, y in zip(input_test, label_test):\n",
    "            self.total_classification += 1\n",
    "            x_bar = np.array(np.insert(x,0,1))\n",
    "            y_hat = self.predict(x_bar , self.params)\n",
    "            y_binary = 1.0 if y == self.class_of_interest else 0.0\n",
    "            \n",
    "            # considering threshold probability = 0.5\n",
    "            if y_hat >=0.5 and y_binary == 1:  #predicted probability > 0.5 and if y is same as our class of interest\n",
    "                self.correct_classification += 1\n",
    "            \n",
    "            if y_hat < 0.5 and y_binary != 1:  #predicted probability < 0.5 and if y is NOT same as our class of interest\n",
    "                self.correct_classification += 1\n",
    "        \n",
    "        self.accuracy = self.correct_classification / self.total_classification\n",
    "    \n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the data in to train and test\n",
    "digits_train, digits_test, digits_label_train, digits_label_test = train_test_split(digits.data,digits.target,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for digit: 0\n",
      "Cost: -1.846328622884904, Iteration: 1000\n",
      "Cost: -1.0372192871263992, Iteration: 2000\n",
      "Cost: -0.729206981163873, Iteration: 3000\n",
      "Cost: -0.5647111806365639, Iteration: 4000\n",
      "Cost: -0.4618231006867816, Iteration: 5000\n",
      "Cost: -0.3911854300348703, Iteration: 6000\n",
      "Cost: -0.3395965741720156, Iteration: 7000\n",
      "Cost: -0.3002192967939829, Iteration: 8000\n",
      "Cost: -0.2691499966537756, Iteration: 9000\n"
     ]
    }
   ],
   "source": [
    "# Training the classifier for ZERO digit\n",
    "print_iter=1000\n",
    "class_of_interest=0\n",
    "alpha = 1e-2\n",
    "max_iter = 10000\n",
    "# Assumption of initial values of parameters\n",
    "initial_param_value=np.zeros(len(digits.data[0])+1)\n",
    "# Initializing LogisticRegression() class\n",
    "dig_log_reg=LogisticRegression()\n",
    "# Setting the class variables\n",
    "dig_log_reg.set_values(initial_param_value,alpha,max_iter,class_of_interest)\n",
    "# Training the model\n",
    "parameters = dig_log_reg.train(digits_train/16.0,digits_label_train,print_iter) #we divided the train data by 16 to normalize it as the maximum value in training data column is 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediciting a Zero digit in test set: 0.9972222222222222\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy on test data\n",
    "digits_accuracy=dig_log_reg.test(digits_test/16.0,digits_label_test)\n",
    "print(f'Accuracy of prediciting a Zero digit in test set: {digits_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_digit(digit):\n",
    "    # Training the classifier for ZERO digit\n",
    "    print_iter=1000\n",
    "    class_of_interest=digit\n",
    "    alpha = 1e-2\n",
    "    max_iter = 10000\n",
    "    # Assumption of initial values of parameters\n",
    "    initial_param_value=np.zeros(len(digits.data[0])+1)\n",
    "    # Initializing LogisticRegression() class\n",
    "    dig_logreg=LogisticRegression()\n",
    "    # Setting the class variables\n",
    "    dig_logreg.set_values(initial_param_value,alpha,max_iter,class_of_interest)\n",
    "    # Training the model\n",
    "    parameters = dig_logreg.train(digits_train/16.0,digits_label_train,print_iter) #we divided the train data by 16 to normalize it as the maximum value in training data column is 16\n",
    "    # Calculate Accuracy\n",
    "    digits_accuracy=dig_logreg.test(digits_test/16.0,digits_label_test)\n",
    "    print(f'-------------------------------------------------------------------------------------')\n",
    "    print(f'Accuracy of prediciting a {class_of_interest} digit in test set: {digits_accuracy}')\n",
    "    return digits_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for digit: 1\n",
      "Cost: -31.001711361197675, Iteration: 1000\n",
      "Cost: -24.382166323027484, Iteration: 2000\n",
      "Cost: -21.028330253945015, Iteration: 3000\n",
      "Cost: -18.852812374623827, Iteration: 4000\n",
      "Cost: -17.268169403144668, Iteration: 5000\n",
      "Cost: -16.032605000590873, Iteration: 6000\n",
      "Cost: -15.025546285383262, Iteration: 7000\n",
      "Cost: -14.179015936199473, Iteration: 8000\n",
      "Cost: -13.451199449110883, Iteration: 9000\n",
      "-------------------------------------------------------------------------------------\n",
      "Accuracy of prediciting a 1 digit in test set: 0.9777777777777777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_digit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for digit: 9\n",
      "Cost: -32.489140253479896, Iteration: 1000\n",
      "Cost: -27.402771720948316, Iteration: 2000\n",
      "Cost: -24.933287105045686, Iteration: 3000\n",
      "Cost: -23.359959942641144, Iteration: 4000\n",
      "Cost: -22.228988755001335, Iteration: 5000\n",
      "Cost: -21.35697973625737, Iteration: 6000\n",
      "Cost: -20.65216135105316, Iteration: 7000\n",
      "Cost: -20.062451504162517, Iteration: 8000\n",
      "Cost: -19.555752222607335, Iteration: 9000\n"
     ]
    }
   ],
   "source": [
    "train_test_digit(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hemen\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\hemen\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.fit(digits_train/16.0,digits_label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logisticRegr.predict(digits_test/16.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "score = logisticRegr.score(digits_test/16.0, digits_label_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc=accuracy_score(digits_label_test,predictions)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
